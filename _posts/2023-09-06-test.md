---
layout : single
categories : ML/DL
title : "혁펜하임 딥러닝 1강"
---

# Gradient, Linear regression

지도 학습( 예_classfication ) 과 비지도 학습( 예_강화학습 )"""


선형 회귀(Linear regression)""

찾고자하는 것은 Ax = y에서의 x"
찾는 방법은 Gradient descent(천천히 찾음), Newton-Raphson(신호처리, 잘안씀), Least-Squares(한번에 찾음)"
Gradient에서 기울기, 미분값은 늘 가장 가파른 방향을 가르키기에 반대로 나아가면 |Ax-y|^2의 값이 최소가 되게하는 x를 찾을 수 있음"
Newrton은 2번 미분해야되기에 잘 사용안함""

왜 Linear인가?""

관점이 중요, ax^2+bx+c라 했을때 x기준으로 보면 비선형이지만, X^2과 x를 분리해서 보면 선형이다.
